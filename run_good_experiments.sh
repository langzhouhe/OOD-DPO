#!/usr/bin/env python3

import sys
import subprocess
import os
import json
import numpy as np
import argparse
import threading
import pandas as pd

def filter_output(process):
    """è¿‡æ»¤stderrä¸­çš„pandasè­¦å‘Š"""
    while True:
        line = process.stderr.readline()
        if line == '' and process.poll() is not None:
            break
        if line:
            # è¿‡æ»¤æ‰pandasè­¦å‘Š
            if not any(keyword in line for keyword in [
                "Failed to find the pandas get_adjustment",
                "Failed to patch pandas", 
                "PandasTools will have limited functionality",
                "RDKit WARNING",
                "UserWarning"
            ]):
                print(line.rstrip(), file=sys.stderr)

def find_model_in_outputs(base_dir="./outputs"):
    """é€’å½’æŸ¥æ‰¾æœ€æ–°çš„æ¨¡å‹æ–‡ä»¶"""
    best_model = None
    latest_time = 0
    
    for root, dirs, files in os.walk(base_dir):
        for file in files:
            if 'best' in file and file.endswith('.pth'):
                file_path = os.path.join(root, file)
                if os.path.exists(file_path):
                    file_time = os.path.getmtime(file_path)
                    if file_time > latest_time:
                        latest_time = file_time
                        best_model = file_path
    
    return best_model

def run_experiment(dataset, domain='size', shift='covariate', seed=42, epochs=500, 
                   batch_size=256, lr=1e-4, foundation_model='minimol', data_seed=42):
    """è¿è¡Œå•æ¬¡è®­ç»ƒ-è¯„ä¼°å®éªŒï¼Œæ”¯æŒminimolå’Œunimol"""
    
    print(f"ğŸš€ å¼€å§‹å®éªŒ: {dataset} (domain={domain}, shift={shift}, seed={seed}, model={foundation_model})")
    
    # æ„å»ºè¾“å‡ºç›®å½•
    experiment_name = f"{dataset}_{domain}_{shift}"
    output_dir = f"./outputs/{foundation_model}/{experiment_name}/{seed}"
    os.makedirs(output_dir, exist_ok=True)
    
    # ç¯å¢ƒå˜é‡è®¾ç½®
    env = os.environ.copy()
    env['TQDM_DISABLE'] = '0'
    env['SHOW_PROGRESS'] = '1'
    env['PYTHONWARNINGS'] = 'ignore'
    env['RDK_QUIET'] = '1'
    
    # æ ¹æ®foundation_modelè®¾ç½®æ‰¹å¤§å°ï¼ˆå‚è€ƒç¬¬ä¸€ä¸ªè„šæœ¬çš„é€»è¾‘ï¼‰
    if foundation_model == "unimol":
        train_batch_size = "256"
        eval_batch_size = "128"
    else:  # minimol
        train_batch_size = "512"
        eval_batch_size = "256"
    
    # å¦‚æœç”¨æˆ·æŒ‡å®šäº†batch_sizeï¼Œä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„å€¼
    if batch_size != 256:  # 256æ˜¯é»˜è®¤å€¼
        train_batch_size = str(batch_size)
        eval_batch_size = str(batch_size // 2)
    
    # è®­ç»ƒå‘½ä»¤ - æ·»åŠ foundation_modelç›¸å…³å‚æ•°
    train_cmd = [
        sys.executable, "main.py",
        "--mode", "train",
        "--dataset", dataset,
        "--good_domain", domain,
        "--good_shift", shift,
        "--foundation_model", foundation_model,
        "--seed", str(seed),
        "--data_seed", str(data_seed),
        "--output_dir", output_dir,
        "--epochs", str(epochs),
        "--batch_size", train_batch_size,
        "--eval_batch_size", eval_batch_size,
        "--lr", str(lr),
        "--eval_steps", "25",
        "--precompute_features",
        "--cache_root", "/home/ubuntu/projects",
        "--encoding_batch_size", "50"
    ]
    
    try:
        print(f"  ğŸ“š å¼€å§‹è®­ç»ƒ ({foundation_model.upper()}, batch={train_batch_size})...")
        
        # ä½¿ç”¨Popenå’Œè¿‡æ»¤çº¿ç¨‹
        process = subprocess.Popen(
            train_cmd,
            env=env,
            stdout=sys.stdout,
            stderr=subprocess.PIPE,
            universal_newlines=True,
            bufsize=1
        )
        
        # å¯åŠ¨è¿‡æ»¤çº¿ç¨‹
        filter_thread = threading.Thread(target=filter_output, args=(process,))
        filter_thread.daemon = True
        filter_thread.start()
        
        # ç­‰å¾…å®Œæˆ
        return_code = process.wait()
        
        if return_code != 0:
            print(f"  âŒ è®­ç»ƒå¤±è´¥ï¼Œé€€å‡ºç : {return_code}")
            return None
            
        print("  âœ… è®­ç»ƒå®Œæˆ")
        
    except Exception as e:
        print(f"  âŒ è®­ç»ƒå¤±è´¥: {e}")
        return None
    
    # æŸ¥æ‰¾æ¨¡å‹æ–‡ä»¶
    model_path = find_model_in_outputs(output_dir)
    
    if not model_path:
        print("  âŒ æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶")
        return None
    
    print(f"  ğŸ“ ä½¿ç”¨æ¨¡å‹: {os.path.basename(model_path)}")
    print("  ğŸ” å¼€å§‹è¯„ä¼°...")
    
    # è¯„ä¼°å‘½ä»¤ - æ·»åŠ foundation_modelç›¸å…³å‚æ•°
    eval_cmd = [
        sys.executable, "main.py",
        "--mode", "eval",
        "--dataset", dataset,
        "--good_domain", domain,
        "--good_shift", shift,
        "--foundation_model", foundation_model,
        "--seed", str(seed),
        "--data_seed", str(data_seed),
        "--model_path", model_path,
        "--output_dir", output_dir,
        "--eval_batch_size", eval_batch_size,
        "--precompute_features",
        "--cache_root", "/home/ubuntu/projects"
    ]
    
    try:
        # è¯„ä¼°ä¹Ÿç”¨ç›¸åŒçš„è¿‡æ»¤æ–¹å¼
        process = subprocess.Popen(
            eval_cmd,
            env=env,
            stdout=sys.stdout,
            stderr=subprocess.PIPE,
            universal_newlines=True,
            bufsize=1
        )
        
        # å¯åŠ¨è¿‡æ»¤çº¿ç¨‹
        filter_thread = threading.Thread(target=filter_output, args=(process,))
        filter_thread.daemon = True
        filter_thread.start()
        
        # ç­‰å¾…å®Œæˆ
        return_code = process.wait()
        
        if return_code != 0:
            print(f"  âŒ è¯„ä¼°å¤±è´¥ï¼Œé€€å‡ºç : {return_code}")
            return None
            
        print("  âœ… è¯„ä¼°å®Œæˆ")
        
    except Exception as e:
        print(f"  âŒ è¯„ä¼°å¤±è´¥: {e}")
        return None
    
    # è¯»å–ç»“æœæ–‡ä»¶ï¼ˆä¼˜å…ˆè¯»å–evaluation_metrics.csvï¼‰
    eval_metrics_file = os.path.join(output_dir, "evaluation_metrics.csv")
    if os.path.exists(eval_metrics_file):
        try:
            metrics_df = pd.read_csv(eval_metrics_file)
            if len(metrics_df) > 0:
                row = metrics_df.iloc[0]
                
                metrics = {
                    'auroc': row.get('auroc', None),
                    'aupr': row.get('aupr', None), 
                    'fpr95': row.get('fpr95', None)
                }
                
                for metric_name, metric_value in metrics.items():
                    if metric_value is not None:
                        print(f"  ğŸ“ˆ {metric_name.upper()}: {metric_value:.4f}")
                
                return metrics
                
        except Exception as e:
            print(f"  âŒ è¯»å–evaluation_metrics.csvå¤±è´¥: {e}")
    
    # å¤‡é€‰ï¼šè¯»å–ood_evaluation_results.jsonï¼ˆå‚è€ƒç¬¬ä¸€ä¸ªè„šæœ¬ï¼‰
    ood_results_file = os.path.join(output_dir, "ood_evaluation_results.json")
    if os.path.exists(ood_results_file):
        try:
            with open(ood_results_file, 'r') as f:
                results = json.load(f)
            
            metrics = {
                'auroc': results.get("auroc"),
                'aupr': results.get("aupr"), 
                'fpr95': results.get("fpr95")
            }
            
            for metric_name, metric_value in metrics.items():
                if metric_value is not None:
                    print(f"  ğŸ“ˆ {metric_name.upper()}: {metric_value:.4f}")
            
            return metrics
                
        except Exception as e:
            print(f"  âŒ è¯»å–ood_evaluation_results.jsonå¤±è´¥: {e}")
    
    # å¤‡é€‰ï¼šè¯»å–eval_results.json
    eval_results_file = os.path.join(output_dir, "eval_results.json")
    if os.path.exists(eval_results_file):
        try:
            with open(eval_results_file, 'r') as f:
                results = json.load(f)
            
            metrics = {
                'auroc': results.get('auroc', results.get('auc', None)),
                'aupr': results.get('aupr', results.get('auprc', None)),
                'fpr95': results.get('fpr95', results.get('fpr_at_95_tpr', None))
            }
            
            for metric_name, metric_value in metrics.items():
                if metric_value is not None:
                    print(f"  ğŸ“ˆ {metric_name.upper()}: {metric_value:.4f}")
            
            return metrics
            
        except Exception as e:
            print(f"  âŒ è¯»å–eval_results.jsonå¤±è´¥: {e}")
    
    # å¦‚æœæ‰€æœ‰ç»“æœæ–‡ä»¶éƒ½ä¸å­˜åœ¨ï¼Œåˆ—å‡ºè¾“å‡ºç›®å½•ä¸­çš„æ–‡ä»¶
    print(f"  ğŸ“‹ è¾“å‡ºç›®å½•ä¸­çš„æ–‡ä»¶:")
    try:
        for file in os.listdir(output_dir):
            print(f"    - {file}")
    except:
        pass
    
    print(f"  âŒ æœªæ‰¾åˆ°ç»“æœæ–‡ä»¶")
    return None

def calculate_stats(values):
    """è®¡ç®—å¹³å‡å€¼å’Œæ ‡å‡†å·®"""
    if not values:
        return None, None

    values = [v for v in values if v is not None]
    if not values:
        return None, None

    mean_val = np.mean(values)
    std_val = np.std(values, ddof=1) if len(values) > 1 else 0.0
    return mean_val, std_val

def generate_dataset_metrics_file(foundation_model, dataset, domain, shift, auroc_mean, auroc_std, aupr_mean, aupr_std, fpr95_mean, fpr95_std, num_seeds):
    """Generate dataset metrics JSON file"""
    import datetime

    experiment_name = f"{dataset}_{domain}_{shift}"
    metrics_dir = f"./outputs/{foundation_model}/{experiment_name}"
    os.makedirs(metrics_dir, exist_ok=True)

    metrics_file = os.path.join(metrics_dir, "dataset_metrics.json")

    metrics_data = {
        "foundation_model": foundation_model,
        "dataset": dataset,
        "domain": domain,
        "shift": shift,
        "experiment_name": experiment_name,
        "metrics": {
            "auroc": {
                "mean": float(auroc_mean) if auroc_mean is not None else None,
                "std": float(auroc_std) if auroc_std is not None else None
            },
            "aupr": {
                "mean": float(aupr_mean) if aupr_mean is not None else None,
                "std": float(aupr_std) if aupr_std is not None else None
            },
            "fpr95": {
                "mean": float(fpr95_mean) if fpr95_mean is not None else None,
                "std": float(fpr95_std) if fpr95_std is not None else None
            }
        },
        "num_seeds": num_seeds,
        "timestamp": datetime.datetime.now().isoformat()
    }

    with open(metrics_file, 'w') as f:
        json.dump(metrics_data, f, indent=2)

    print(f"ğŸ“„ Dataset metrics saved to: {metrics_file}")

def parse_args():
    parser = argparse.ArgumentParser(description="è¿è¡Œgood_dataæ•°æ®é›†å¤šç§å­å®éªŒï¼ˆæ”¯æŒminimolå’Œunimolï¼‰")
    
    parser.add_argument("--datasets", nargs='+', 
                        choices=['good_hiv', 'good_pcba', 'good_zinc'],
                        default=['good_hiv', 'good_pcba', 'good_zinc'],
                        help="è¦è¿è¡Œçš„æ•°æ®é›†")
    
    parser.add_argument("--domains", nargs='+',
                        choices=['scaffold', 'size'], 
                        default=['size', 'scaffold'],
                        help="è¦æµ‹è¯•çš„åŸŸ")
    
    parser.add_argument("--shifts", nargs='+',
                        choices=['covariate', 'concept', 'no_shift'],
                        default=['covariate'],
                        help="è¦æµ‹è¯•çš„shiftç±»å‹")
    
    parser.add_argument("--seeds", nargs='+', type=int,
                        default=[1, 2, 3, 4, 5, 6,7,8,9,10],
                        help="è¦æµ‹è¯•çš„éšæœºç§å­")
    
    # ğŸ”¥ ä¿®å¤ï¼šæ·»åŠ foundation_modelså‚æ•°æ”¯æŒ
    parser.add_argument("--foundation_models", nargs='+',
                        choices=['minimol', 'unimol'],
                        default=['minimol', 'unimol'],
                        help="è¦æµ‹è¯•çš„åŸºç¡€æ¨¡å‹")
    
    # ğŸ”¥ å…¼å®¹æ€§ï¼šåŒæ—¶æ”¯æŒå•æ•°å½¢å¼ï¼ˆå‘åå…¼å®¹ï¼‰
    parser.add_argument("--foundation_model", type=str,
                        choices=['minimol', 'unimol'],
                        default=None,
                        help="å•ä¸ªåŸºç¡€æ¨¡å‹ï¼ˆå‘åå…¼å®¹ï¼‰")
    
    parser.add_argument("--data_seed", type=int, default=42,
                        help="æ•°æ®åˆ’åˆ†éšæœºç§å­")
    
    parser.add_argument("--epochs", type=int, default=500, help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--batch_size", type=int, default=256, help="æ‰¹å¤§å°ï¼ˆå¯é€‰ï¼Œä¼šè¢«æ¨¡å‹é»˜è®¤å€¼è¦†ç›–ï¼‰")
    parser.add_argument("--lr", type=float, default=1e-4, help="å­¦ä¹ ç‡")
    
    return parser.parse_args()

def main():
    args = parse_args()
    
    # ğŸ”¥ ä¿®å¤ï¼šå¤„ç†å‚æ•°å…¼å®¹æ€§
    if args.foundation_model is not None:
        # å¦‚æœæŒ‡å®šäº†å•æ•°å½¢å¼ï¼Œè½¬æ¢ä¸ºåˆ—è¡¨å½¢å¼
        args.foundation_models = [args.foundation_model]
        print(f"âš ï¸  æ£€æµ‹åˆ°å•æ¨¡å‹å‚æ•°ï¼Œè‡ªåŠ¨è½¬æ¢: {args.foundation_model}")
    
    print("ğŸ¯ Good Data å¤šç§å­å¤šæ¨¡å‹å®éªŒè¿è¡Œå™¨")
    print("ğŸ”‡ å·²è‡ªåŠ¨è¿‡æ»¤RDKit pandasè­¦å‘Š")
    print("=" * 80)
    print(f"æ•°æ®é›†: {args.datasets}")
    print(f"åŸŸ: {args.domains}")
    print(f"Shift: {args.shifts}")
    print(f"åŸºç¡€æ¨¡å‹: {args.foundation_models}")
    print(f"ç§å­: {args.seeds}")
    print(f"æ•°æ®ç§å­: {args.data_seed}")
    print(f"è®­ç»ƒå‚æ•°: epochs={args.epochs}, lr={args.lr}")
    print(f"ğŸ”§ æ¨¡å‹ç‰¹å®šæ‰¹å¤§å°: unimol(256/128), minimol(512/256)")
    print("=" * 80)
    
    all_results = {}
    total_experiments = len(args.datasets) * len(args.domains) * len(args.shifts) * len(args.foundation_models) * len(args.seeds)
    completed_experiments = 0
    
    for foundation_model in args.foundation_models:
        print(f"\n{'ğŸ¤– åŸºç¡€æ¨¡å‹: ' + foundation_model.upper():<80}")
        
        for dataset in args.datasets:
            for domain in args.domains:
                for shift in args.shifts:
                    experiment_name = f"{foundation_model}_{dataset}_{domain}_{shift}"
                    print(f"\n{'='*60}")
                    print(f"ğŸ¯ å®éªŒé…ç½®: {experiment_name}")
                    print(f"{'='*60}")
                    
                    # æ”¶é›†æ¯ä¸ªæŒ‡æ ‡çš„æ‰€æœ‰ç§å­ç»“æœ
                    experiment_metrics = {
                        'auroc': [],
                        'aupr': [],
                        'fpr95': []
                    }
                    
                    successful_runs = 0
                    for seed in args.seeds:
                        completed_experiments += 1
                        print(f"\nğŸŒ± ç§å­ {seed} [{completed_experiments}/{total_experiments}]:")
                        
                        metrics = run_experiment(
                            dataset=dataset,
                            domain=domain,
                            shift=shift,
                            seed=seed,
                            epochs=args.epochs,
                            batch_size=args.batch_size,
                            lr=args.lr,
                            foundation_model=foundation_model,
                            data_seed=args.data_seed
                        )
                        
                        if metrics is not None:
                            successful_runs += 1
                            for metric_name, metric_value in metrics.items():
                                if metric_value is not None:
                                    experiment_metrics[metric_name].append(metric_value)
                        else:
                            print(f"  ğŸ’¥ ç§å­ {seed} å®éªŒå¤±è´¥")
                    
                    # è®¡ç®—æ¯ä¸ªæŒ‡æ ‡çš„ç»Ÿè®¡é‡
                    if successful_runs > 0:
                        experiment_stats = {}
                        for metric_name, metric_values in experiment_metrics.items():
                            mean_val, std_val = calculate_stats(metric_values)
                            experiment_stats[metric_name] = {
                                'mean': mean_val,
                                'std': std_val,
                                'count': len(metric_values)
                            }

                        all_results[experiment_name] = experiment_stats

                        # Generate individual dataset metrics JSON file
                        auroc_mean = experiment_stats['auroc']['mean']
                        auroc_std = experiment_stats['auroc']['std']
                        aupr_mean = experiment_stats['aupr']['mean']
                        aupr_std = experiment_stats['aupr']['std']
                        fpr95_mean = experiment_stats['fpr95']['mean']
                        fpr95_std = experiment_stats['fpr95']['std']

                        generate_dataset_metrics_file(
                            foundation_model, dataset, domain, shift,
                            auroc_mean, auroc_std, aupr_mean, aupr_std, fpr95_mean, fpr95_std,
                            successful_runs
                        )

                        # æ‰“å°å½“å‰å®éªŒæ€»ç»“
                        print(f"\nğŸ“Š {experiment_name} æ€»ç»“ (æˆåŠŸè¿è¡Œ: {successful_runs}/{len(args.seeds)}):")
                        for metric_name, stats in experiment_stats.items():
                            if stats['mean'] is not None:
                                print(f"  {metric_name.upper()}: {stats['mean']:.4f} Â± {stats['std']:.4f} (n={stats['count']})")
                            else:
                                print(f"  {metric_name.upper()}: æ— æœ‰æ•ˆæ•°æ®")
                    else:
                        print(f"\nğŸ’¥ {experiment_name}: æ‰€æœ‰å®éªŒéƒ½å¤±è´¥äº†")
    
    # æœ€ç»ˆå®Œæ•´æŠ¥å‘Š
    print(f"\n{'='*100}")
    print("ğŸš€ Good Data å¤šæ¨¡å‹å®éªŒæœ€ç»ˆæ€»ç»“æŠ¥å‘Š ğŸš€".center(100))
    print(f"{'='*100}")
    
    if all_results:
        # æŒ‰æ¨¡å‹åˆ†ç»„æ˜¾ç¤º
        for foundation_model in args.foundation_models:
            print(f"\nğŸ¤– {foundation_model.upper()} æ¨¡å‹ç»“æœ:")
            print(f"{'å®éªŒé…ç½®':<50} {'AUROC':<15} {'AUPR':<15} {'FPR95':<15}")
            print(f"{'-'*50} {'-'*15} {'-'*15} {'-'*15}")
            
            model_results = {k: v for k, v in all_results.items() if k.startswith(foundation_model)}
            for experiment_name, experiment_stats in model_results.items():
                # ç§»é™¤æ¨¡å‹åå‰ç¼€æ˜¾ç¤º
                display_name = experiment_name[len(foundation_model)+1:]
                
                auroc_str = f"{experiment_stats['auroc']['mean']:.3f}Â±{experiment_stats['auroc']['std']:.3f}" if experiment_stats['auroc']['mean'] is not None else "N/A"
                aupr_str = f"{experiment_stats['aupr']['mean']:.3f}Â±{experiment_stats['aupr']['std']:.3f}" if experiment_stats['aupr']['mean'] is not None else "N/A"
                fpr95_str = f"{experiment_stats['fpr95']['mean']:.3f}Â±{experiment_stats['fpr95']['std']:.3f}" if experiment_stats['fpr95']['mean'] is not None else "N/A"
                
                print(f"{display_name:<50} {auroc_str:<15} {aupr_str:<15} {fpr95_str:<15}")
        
        # ä¿å­˜è¯¦ç»†ç»“æœåˆ°CSVæ–‡ä»¶
        results_file = "./good_experiment_results_multi_model.csv"
        summary_data = []
        
        for experiment_name, experiment_stats in all_results.items():
            # è§£æå®éªŒåç§°: foundation_model_dataset_domain_shift
            parts = experiment_name.split('_')
            if len(parts) >= 4:
                foundation_model = parts[0]
                dataset = parts[1] + '_' + parts[2]  # good_hiv, good_pcba, good_zinc
                domain = parts[3]
                shift = parts[4] if len(parts) > 4 else 'unknown'
            else:
                foundation_model, dataset, domain, shift = 'unknown', experiment_name, 'unknown', 'unknown'
            
            row = {
                'experiment': experiment_name,
                'foundation_model': foundation_model,
                'dataset': dataset,
                'domain': domain, 
                'shift': shift
            }
            
            for metric_name, stats in experiment_stats.items():
                if stats['mean'] is not None:
                    row[f'{metric_name}_mean'] = stats['mean']
                    row[f'{metric_name}_std'] = stats['std']
                    row[f'{metric_name}_count'] = stats['count']
                else:
                    row[f'{metric_name}_mean'] = None
                    row[f'{metric_name}_std'] = None
                    row[f'{metric_name}_count'] = 0
            summary_data.append(row)
        
        summary_df = pd.DataFrame(summary_data)
        summary_df.to_csv(results_file, index=False)
        print(f"\nğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {results_file}")
        
        # ä¿å­˜JSONæ ¼å¼çš„å®Œæ•´ç»“æœï¼ˆå‚è€ƒç¬¬ä¸€ä¸ªè„šæœ¬ï¼‰
        json_file = "./good_experiment_results_complete.json"
        complete_results = {
            'foundation_models': args.foundation_models,
            'experimental_setup': {
                'datasets': args.datasets,
                'domains': args.domains,
                'shifts': args.shifts,
                'train_seeds': args.seeds,
                'data_seed': args.data_seed,
                'epochs': args.epochs,
                'lr': args.lr
            },
            'results': all_results,
            'total_experiments': total_experiments,
            'completed_experiments': completed_experiments
        }
        
        with open(json_file, 'w') as f:
            json.dump(complete_results, f, indent=2)
        print(f"ğŸ’¾ å®Œæ•´ç»“æœå·²ä¿å­˜åˆ°: {json_file}")
        
    else:
        print("ğŸ’¥ æ²¡æœ‰æˆåŠŸçš„å®éªŒç»“æœ")
    
    print(f"{'='*100}")

if __name__ == "__main__":
    main()