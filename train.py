import os
import json
import glob
import logging
import torch
import torch.optim as optim
from tqdm import tqdm
from data_loader import EnergyDPODataLoader
from model import create_model
from sklearn.metrics import roc_auc_score, precision_recall_curve, auc, roc_curve

logger = logging.getLogger(__name__)

def parse_args():
    import argparse
    parser = argparse.ArgumentParser()
    
    # Data
    parser.add_argument("--dataset", type=str, default="drugood")
    parser.add_argument("--drugood_subset", type=str, default="lbap_general_ec50_scaffold")
    parser.add_argument("--data_path", type=str, default="./data")
    parser.add_argument("--data_file", type=str, help="Specific data file path")

    # Model
    parser.add_argument("--hidden_dim", type=int, default=256)
    parser.add_argument("--model_path", type=str, default=None, help="Checkpoint path for resuming training")

    # Training
    parser.add_argument("--dpo_beta", type=float, default=0.1)
    parser.add_argument("--epochs", type=int, default=50)
    parser.add_argument("--batch_size", type=int, default=32)
    parser.add_argument("--eval_batch_size", type=int, default=64)
    parser.add_argument("--lr", type=float, default=1e-4)
    parser.add_argument("--weight_decay", type=float, default=1e-3)
    parser.add_argument("--grad_clip", type=float, default=1.0)

    # Others
    parser.add_argument("--device", type=str, default="cuda")
    parser.add_argument("--seed", type=int, default=42)
    parser.add_argument("--output_dir", type=str, default="./outputs")
    parser.add_argument("--eval_steps", type=int, default=200)
    parser.add_argument("--save_steps", type=int, default=1000)
    
    return parser.parse_args()

class EnergyDPOTrainer:
    def __init__(self, args):

        self.args = args
        
        # 1. First set device
        self.device = torch.device(args.device if torch.cuda.is_available() else 'cpu')

        # 2. Create output directory
        os.makedirs(args.output_dir, exist_ok=True)

        # 3. Create model
        self.model = create_model(args).to(self.device)

        # 4. Create optimizer and scheduler
        self.optimizer = optim.AdamW(
            self.model.parameters(),
            lr=args.lr,
            weight_decay=args.weight_decay
        )
        
        self.scheduler = optim.lr_scheduler.StepLR(
            self.optimizer, 
            step_size=10, 
            gamma=0.9
        )

        # 5. Initialize training state (set only once)
        self.start_epoch = 0
        self.global_step = 0
        self.best_eval_metric = float('-inf')
        self.patience = getattr(self.args, 'early_stopping_patience', 10)
        self.epochs_no_improve = 0

        # 6. Training dynamics tracking for "hard pairs corrected first" analysis
        self.training_dynamics = []

        # 6. Finally load checkpoint (if exists)
        self.load_checkpoint_if_exists()

        logger.info(f"Trainer initialization complete")
        logger.info(f"Starting epoch: {self.start_epoch + 1}")
        logger.info(f"Device: {self.device}")
    
    def load_checkpoint_if_exists(self):
        """Check and load checkpoint"""
        if hasattr(self.args, 'model_path') and self.args.model_path and os.path.exists(self.args.model_path):
            try:
                logger.info(f"Loading checkpoint: {self.args.model_path}")
                checkpoint = torch.load(self.args.model_path, map_location=self.device)
                
                # åŠ è½½æ¨¡å‹çŠ¶æ€
                if 'model_state_dict' in checkpoint:
                    self.model.load_state_dict(checkpoint['model_state_dict'])
                    logger.info("Model state loaded")
                
                # åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€  
                if 'optimizer_state_dict' in checkpoint:
                    self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
                    logger.info("âœ… ä¼˜åŒ–å™¨çŠ¶æ€å·²åŠ è½½")
                
                # åŠ è½½è°ƒåº¦å™¨çŠ¶æ€
                if 'scheduler_state_dict' in checkpoint:
                    self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
                    logger.info("âœ… è°ƒåº¦å™¨çŠ¶æ€å·²åŠ è½½")
                
                # åŠ è½½è®­ç»ƒçŠ¶æ€
                if 'epoch' in checkpoint:
                    self.start_epoch = checkpoint['epoch'] + 1  # ä»ä¸‹ä¸€ä¸ªepochå¼€å§‹
                    logger.info(f"âœ… å°†ä»ç¬¬ {self.start_epoch + 1} ä¸ªepochç»§ç»­è®­ç»ƒ")
                
                if 'global_step' in checkpoint:
                    self.global_step = checkpoint['global_step']
                    logger.info(f"âœ… Global step: {self.global_step}")
                
                if 'best_eval_metric' in checkpoint:
                    self.best_eval_metric = checkpoint['best_eval_metric']
                    logger.info(f"âœ… æœ€ä½³æŒ‡æ ‡: {self.best_eval_metric:.4f}")
                
                logger.info(f"ğŸ‰ æˆåŠŸä»checkpointæ¢å¤è®­ç»ƒçŠ¶æ€ï¼")
                
            except Exception as e:
                logger.error(f"âŒ åŠ è½½checkpointå¤±è´¥: {e}")
                logger.info("ğŸ”„ å°†ä»å¤´å¼€å§‹è®­ç»ƒ")
                self.start_epoch = 0
                self.global_step = 0
                self.best_eval_metric = float('-inf')
        else:
            if hasattr(self.args, 'model_path') and self.args.model_path:
                logger.warning(f"âš ï¸ Checkpointæ–‡ä»¶ä¸å­˜åœ¨: {self.args.model_path}")
            logger.info("ğŸš€ å¼€å§‹å…¨æ–°è®­ç»ƒ")
    
    def compute_energy_dpo_loss(self, id_smiles=None, ood_smiles=None, batch_data=None):
        """æ”¯æŒé¢„è®¡ç®—ç‰¹å¾çš„DPOæŸå¤±è®¡ç®—"""
        if batch_data is not None:
            # ç›´æ¥ä¼ é€’ç»™æ¨¡å‹ï¼Œæ¨¡å‹ä¼šè‡ªåŠ¨é€‰æ‹©è·¯å¾„
            return self.model(batch_data)
        else:
            # å‘åå…¼å®¹
            batch_data = {'id_smiles': id_smiles, 'ood_smiles': ood_smiles}
            return self.model(batch_data)
    
    def evaluate(self, eval_dataloader):
        """è¯„ä¼°æ¨¡å‹"""
        self.model.eval()
        total_loss = 0
        energy_separations = []
        all_id_scores = []
        all_ood_scores = []
        
        with torch.no_grad():
            for batch_data in eval_dataloader:
                # è®¡ç®—æŸå¤±ï¼ˆç”¨äºæ—¥å¿—ï¼‰
                loss, loss_dict = self.compute_energy_dpo_loss(batch_data=batch_data)
                total_loss += loss.item()
                energy_separations.append(loss_dict['energy_separation'])

                # è®¡ç®—åˆ†æ•°ç”¨äº AUROCï¼ˆåŸºäºèƒ½é‡ï¼ŒOOD åº”é«˜äº IDï¼‰
                try:
                    id_scores = self.model.predict_ood_score_from_features(batch_data['id_features'])
                    ood_scores = self.model.predict_ood_score_from_features(batch_data['ood_features'])
                except Exception:
                    # å›é€€: ç›´æ¥é€šè¿‡èƒ½é‡å¤´è®¡ç®—
                    id_scores = self.model.energy_head(batch_data['id_features'].to(next(self.model.parameters()).device)).squeeze(-1).detach().cpu().numpy()
                    ood_scores = self.model.energy_head(batch_data['ood_features'].to(next(self.model.parameters()).device)).squeeze(-1).detach().cpu().numpy()

                all_id_scores.append(id_scores)
                all_ood_scores.append(ood_scores)
        
        avg_loss = total_loss / max(1, len(eval_dataloader))
        avg_energy_sep = sum(energy_separations) / max(1, len(energy_separations))

        # æ‹¼æ¥å¹¶è®¡ç®— AUROC / AUPR / FPR95
        import numpy as np
        id_scores_np = np.concatenate(all_id_scores) if len(all_id_scores) > 0 else np.array([])
        ood_scores_np = np.concatenate(all_ood_scores) if len(all_ood_scores) > 0 else np.array([])

        if id_scores_np.size > 0 and ood_scores_np.size > 0:
            labels = np.concatenate([np.zeros_like(id_scores_np), np.ones_like(ood_scores_np)])
            scores = np.concatenate([id_scores_np, ood_scores_np])
            try:
                val_auroc = roc_auc_score(labels, scores)
                precision, recall, _ = precision_recall_curve(labels, scores)
                val_aupr = auc(recall, precision)
                fpr, tpr, _ = roc_curve(labels, scores)
                idx = np.where(tpr >= 0.95)[0]
                val_fpr95 = float(fpr[idx[0]]) if len(idx) > 0 else 1.0
            except Exception:
                val_auroc, val_aupr, val_fpr95 = 0.0, 0.0, 1.0
        else:
            val_auroc, val_aupr, val_fpr95 = 0.0, 0.0, 1.0
        
        return {
            'total_loss': avg_loss,
            'energy_separation': avg_energy_sep,
            'val_auroc': val_auroc,
            'val_aupr': val_aupr,
            'val_fpr95': val_fpr95,
        }
    
    def save_training_dynamics(self, epoch_data):
        """Save training dynamics data for "hard pairs corrected first" analysis"""
        import csv
        dynamics_path = os.path.join(self.args.output_dir, 'training_dynamics.csv')

        # Create CSV file with header if it doesn't exist
        file_exists = os.path.exists(dynamics_path)
        with open(dynamics_path, 'a', newline='') as csvfile:
            fieldnames = ['epoch', 'misranked_ratio', 'boundary_ratio', 'avg_margin']
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)

            if not file_exists:
                writer.writeheader()

            writer.writerow(epoch_data)

        logger.info(f"Training dynamics saved: epoch {epoch_data['epoch']}, "
                   f"misranked: {epoch_data['misranked_ratio']:.4f}, "
                   f"boundary: {epoch_data['boundary_ratio']:.4f}, "
                   f"margin: {epoch_data['avg_margin']:.4f}")

    def save_checkpoint(self, epoch, is_best=False):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'global_step': self.global_step,
            'best_eval_metric': self.best_eval_metric,
            'args': self.args
        }
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if is_best:
            filename = 'best_model.pth'
            save_path = os.path.join(self.args.output_dir, filename)
            torch.save(checkpoint, save_path)
            logger.info(f"âœ… æœ€ä½³æ¨¡å‹å·²ä¿å­˜åˆ° {save_path}")
        
        # å®šæœŸä¿å­˜epochæ£€æŸ¥ç‚¹
        if epoch % 5 == 0 or epoch == self.args.epochs - 1:
            filename = f'checkpoint_epoch_{epoch:03d}.pth'
            save_path = os.path.join(self.args.output_dir, filename)
            torch.save(checkpoint, save_path)
            logger.info(f"æ£€æŸ¥ç‚¹å·²ä¿å­˜åˆ° {save_path}")
            
            # æ¸…ç†æ—§æ–‡ä»¶ï¼Œåªä¿ç•™æœ€è¿‘3ä¸ªepochæ–‡ä»¶
            pattern = os.path.join(self.args.output_dir, 'checkpoint_epoch_*.pth')
            old_files = sorted(glob.glob(pattern))
            if len(old_files) > 3:
                for old_file in old_files[:-3]:
                    try:
                        os.remove(old_file)
                        logger.info(f"æ¸…ç†æ—§æ–‡ä»¶: {os.path.basename(old_file)}")
                    except:
                        pass
    
    def train(self):
        """Start training"""
        logger.info("Starting training...")
        logger.info(f"Device: {self.device}")
        logger.info(f"è¾“å‡ºç›®å½•: {self.args.output_dir}")
        logger.info(f"èµ·å§‹epoch: {self.start_epoch + 1}")
        logger.info(f"æ€»epochs: {self.args.epochs}")
        
        # åŠ è½½æ•°æ®
        data_loader = EnergyDPODataLoader(self.args)
        train_dataloader, eval_dataloader = data_loader.get_dataloaders()
        
        # è®­ç»ƒå¾ªç¯ - ğŸ”¥ ä½¿ç”¨self.start_epochä½œä¸ºèµ·å§‹ç‚¹
        for epoch in range(self.start_epoch, self.args.epochs):
            self.model.train()
            train_losses = []
            # Training dynamics data collection
            epoch_dynamics = {'misranked_ratio': [], 'boundary_ratio': [], 'avg_margin': []}
            
            # è®¾ç½®è¿›åº¦æ¡
            use_tqdm = os.getenv('TQDM_DISABLE', '0') == '0'
            if use_tqdm:
                progress_iter = tqdm(train_dataloader, desc=f"ğŸš€ Epoch {epoch+1}/{self.args.epochs}")
            else:
                progress_iter = train_dataloader
                total_batches = len(train_dataloader)
            
            for batch_idx, batch_data in enumerate(progress_iter):
                # ç›´æ¥ä¼ é€’batch_dataï¼Œæ¨¡å‹ä¼šè‡ªåŠ¨é€‰æ‹©æœ€ä¼˜è·¯å¾„
                loss, loss_dict = self.compute_energy_dpo_loss(batch_data=batch_data)
                
                # åå‘ä¼ æ’­
                self.optimizer.zero_grad()
                loss.backward()
                
                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.args.grad_clip)
                
                self.optimizer.step()
                self.global_step += 1
                
                # è®°å½•æŸå¤±
                train_losses.append(loss_dict)

                # Collect training dynamics data if available
                if 'misranked_ratio' in loss_dict:
                    epoch_dynamics['misranked_ratio'].append(loss_dict['misranked_ratio'])
                    epoch_dynamics['boundary_ratio'].append(loss_dict['boundary_ratio'])
                    epoch_dynamics['avg_margin'].append(loss_dict['avg_margin'])
                
                # æ›´æ–°è¿›åº¦æ˜¾ç¤º
                if use_tqdm:
                    progress_iter.set_postfix({
                        'Loss': f"{loss.item():.4f}",
                        'E_Sep': f"{loss_dict['energy_separation']:.4f}"
                    })
                else:
                    # ç®€å•çš„è¿›åº¦æ˜¾ç¤º
                    progress = (batch_idx + 1) / total_batches * 100
                    bar_length = 30
                    filled_length = int(bar_length * (batch_idx + 1) // total_batches)
                    bar = 'â–ˆ' * filled_length + 'â–‘' * (bar_length - filled_length)
                    print(f"\rğŸš€ Epoch {epoch+1}/{self.args.epochs} |{bar}| {progress:5.1f}% ({batch_idx+1}/{total_batches}) "
                          f"Loss: {loss.item():.4f} E_Sep: {loss_dict['energy_separation']:.4f}", end='')
                
                # å®šæœŸè¯„ä¼°
                if self.global_step % self.args.eval_steps == 0:
                    eval_msg = f"ğŸ” æ­¥éª¤ {self.global_step} - å¼€å§‹è¯„ä¼°..."
                    if use_tqdm:
                        progress_iter.write(eval_msg)
                    else:
                        print(f"\n{eval_msg}")
                        logger.info(eval_msg)
                    
                    eval_loss_dict = self.evaluate(eval_dataloader)
                    
                    result_msg = (
                        f"âœ… æ­¥éª¤ {self.global_step} - éªŒè¯æŸå¤±: {eval_loss_dict['total_loss']:.4f}, "
                        f"èƒ½é‡åˆ†ç¦»: {eval_loss_dict['energy_separation']:.4f}, "
                        f"Val-AUROC: {eval_loss_dict['val_auroc']:.4f}"
                    )
                    if use_tqdm:
                        progress_iter.write(result_msg)
                    else:
                        print(result_msg)
                        logger.info(result_msg)
                    
                    # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€ä½³æ¨¡å‹
                    current_metric = eval_loss_dict['val_auroc']
                    if current_metric > self.best_eval_metric:
                        self.best_eval_metric = current_metric
                        self.save_checkpoint(epoch, is_best=True)
                        best_msg = f"ğŸ‰ å‘ç°æ›´å¥½çš„æ¨¡å‹ï¼Val-AUROC: {current_metric:.4f}"
                        if use_tqdm:
                            progress_iter.write(best_msg)
                        else:
                            print(best_msg)
                            logger.info(best_msg)
            
            # å®Œæˆå½“å‰epochçš„è¿›åº¦æ˜¾ç¤º
            if not use_tqdm:
                print()  # Ensure newline
            elif use_tqdm:
                progress_iter.close()
            
            # epochç»“æŸæ—¶çš„è¯„ä¼°ï¼ˆåŸºäº Val-AUROC é€‰æ‹©ï¼‰
            eval_loss_dict = self.evaluate(eval_dataloader)
            
            # è®¡ç®—å¹³å‡è®­ç»ƒæŸå¤±
            avg_train_loss = sum(loss['total_loss'] for loss in train_losses) / len(train_losses)

            # Save training dynamics data at epoch end
            if epoch_dynamics['misranked_ratio']:  # Only if we have dynamics data
                avg_dynamics = {
                    'epoch': epoch + 1,
                    'misranked_ratio': sum(epoch_dynamics['misranked_ratio']) / len(epoch_dynamics['misranked_ratio']),
                    'boundary_ratio': sum(epoch_dynamics['boundary_ratio']) / len(epoch_dynamics['boundary_ratio']),
                    'avg_margin': sum(epoch_dynamics['avg_margin']) / len(epoch_dynamics['avg_margin'])
                }
                self.save_training_dynamics(avg_dynamics)
            
            logger.info(f"Epoch {epoch+1} å®Œæˆ:")
            logger.info(f"  è®­ç»ƒæŸå¤±: {avg_train_loss:.4f}")
            logger.info(f"  éªŒè¯æŸå¤±: {eval_loss_dict['total_loss']:.4f}")
            logger.info(f"  èƒ½é‡åˆ†ç¦»: {eval_loss_dict['energy_separation']:.4f}")
            logger.info(f"  Val-AUROC: {eval_loss_dict['val_auroc']:.4f} (AUPR: {eval_loss_dict['val_aupr']:.4f}, FPR95: {eval_loss_dict['val_fpr95']:.4f})")
            
            # æ›´æ–°å­¦ä¹ ç‡
            self.scheduler.step()
            
            # åŸºäº Val-AUROC çš„æ—©åœä¸æœ€ä½³æ¨¡å‹ä¿å­˜
            current_metric = eval_loss_dict['val_auroc']
            if current_metric > self.best_eval_metric:
                self.best_eval_metric = current_metric
                self.save_checkpoint(epoch, is_best=True)
                self.epochs_no_improve = 0
                logger.info(f"ğŸ‰ epoch å‘ç°æ›´å¥½æ¨¡å‹! Val-AUROC: {current_metric:.4f}")
            
            # ä¿å­˜æœ¬epochçš„æ£€æŸ¥ç‚¹
            self.save_checkpoint(epoch)

            # æ—©åœåˆ¤æ–­
            if current_metric <= self.best_eval_metric + 1e-12:
                self.epochs_no_improve += 1
            if self.epochs_no_improve >= self.patience:
                logger.info(f"â¹ï¸ æ—©åœè§¦å‘: è¿ç»­ {self.epochs_no_improve} ä¸ªepochæ— æå‡ (patience={self.patience})")
                break
        
        logger.info("è®­ç»ƒå®Œæˆï¼")
        logger.info(f"æœ€ä½³ Val-AUROC: {self.best_eval_metric:.4f}")
        
        # ä¿å­˜é…ç½®æ–‡ä»¶
        args_path = os.path.join(self.args.output_dir, 'config.json')
        with open(args_path, 'w') as f:
            json.dump(vars(self.args), f, indent=2, ensure_ascii=False)

def main():
    logging.basicConfig(level=logging.INFO)
    
    args = parse_args()
    
    trainer = EnergyDPOTrainer(args)
    trainer.train()

if __name__ == "__main__":
    main()
