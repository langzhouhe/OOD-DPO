# GPU Memory Optimization Summary

## ä¼˜åŒ–å®Œæˆæ—¶é—´
2025-11-20

## é—®é¢˜è¯Šæ–­

### 1. Batch Sizeè¿‡å°
- **åŸå§‹è®¾ç½®**é’ˆå¯¹å°å‹GPUè®¾è®¡ï¼Œåœ¨80GB A100ä¸Šä¸¥é‡ä½æ•ˆ
- **åŸå§‹encoding batch size**: 50 (æœ€å¤§ç“¶é¢ˆ!)
- **åŸå§‹è®­ç»ƒbatch size**: 512 (minimol), 256 (unimol)
- **åŸå§‹baseline batch size**: 32 (minimol), 16 (unimol)

### 2. GPUå†…å­˜æµ‹é‡ä¸å®Œæ•´
- âŒ **æœªæµ‹é‡**: Foundation model encodingé˜¶æ®µçš„GPUå†…å­˜
- âœ… **å·²æµ‹é‡**: Trainingå’ŒEvaluationé˜¶æ®µçš„GPUå†…å­˜
- **ç»“æœ**: æŠ¥å‘Šçš„GPUä½¿ç”¨é‡ä»…~17MBï¼Œä¸¥é‡ä½ä¼°å®é™…ä½¿ç”¨

## ä¼˜åŒ–æ–¹æ¡ˆ

### A. Batch Sizeä¼˜åŒ– (3ä¸ªæ–‡ä»¶)

#### 1. run_experiments.sh
**ä½ç½®**: `/home/ubuntu/OOD-DPO/run_experiments.sh`

| å‚æ•° | åŸå€¼ (Minimol) | æ–°å€¼ (Minimol) | åŸå€¼ (Unimol) | æ–°å€¼ (Unimol) |
|------|---------------|---------------|--------------|-------------|
| batch_size | 512 | **8192** (16x) ğŸ”¥ | 256 | **4096** (16x) ğŸ”¥ |
| eval_batch_size | 256 | **4096** (16x) ğŸ”¥ | 128 | **4096** (32x) ğŸ”¥ |
| encoding_batch_size | 50 | **500** (10x) | 50 | **500** (10x) |

#### 2. run_baselines.sh
**ä½ç½®**: `/home/ubuntu/OOD-DPO/run_baselines.sh`

| å‚æ•° | åŸå€¼ (Minimol) | æ–°å€¼ (Minimol) | åŸå€¼ (Unimol) | æ–°å€¼ (Unimol) |
|------|---------------|---------------|--------------|-------------|
| BATCH_SIZE | 32 | **1024** (32x) ğŸ”¥ | 16 | **512** (32x) ğŸ”¥ |
| EVAL_BATCH_SIZE | 64 | **2048** (32x) ğŸ”¥ | 32 | **1024** (32x) ğŸ”¥ |
| encoding_batch_size | 50 | **500** (10x) | 50 | **500** (10x) |

#### 3. run_cross_dataset_experiments.sh
**ä½ç½®**: `/home/ubuntu/OOD-DPO/run_cross_dataset_experiments.sh`

| å‚æ•° | åŸå€¼ (Minimol) | æ–°å€¼ (Minimol) | åŸå€¼ (Unimol) | æ–°å€¼ (Unimol) |
|------|---------------|---------------|--------------|-------------|
| batch_size | 512 | **8192** (16x) ğŸ”¥ | 256 | **4096** (16x) ğŸ”¥ |
| eval_batch_size | 256 | **4096** (16x) ğŸ”¥ | 128 | **4096** (32x) ğŸ”¥ |
| encoding_batch_size | 50 | **500** (10x) | 50 | **500** (10x) |

### B. GPUå†…å­˜æµ‹é‡å®Œå–„ (3ä¸ªæ–‡ä»¶)

#### 1. data_loader.py
**ä½ç½®**: `/home/ubuntu/OOD-DPO/data_loader.py`

**ä¿®æ”¹å†…å®¹**:
- åœ¨ `_compute_features_batch()` æ–¹æ³•å¼€å§‹æ—¶è°ƒç”¨ `torch.cuda.reset_peak_memory_stats()`
- åœ¨encodingå®Œæˆåæµ‹é‡å¹¶ä¿å­˜ `peak_encoding_memory_gb`
- è®°å½•åˆ°æ—¥å¿—: `Peak GPU memory during encoding: X.XX GB`

**å…³é”®ä»£ç **:
```python
# Line 560-563: åœ¨encodingå¼€å§‹æ—¶é‡ç½®GPUå†…å­˜ç»Ÿè®¡
if torch.cuda.is_available():
    torch.cuda.reset_peak_memory_stats()
    logger.info("Starting foundation model encoding - GPU memory tracking enabled")

# Line 600-605: åœ¨encodingç»“æŸæ—¶æµ‹é‡peak memory
if torch.cuda.is_available():
    peak_encoding_memory_gb = torch.cuda.max_memory_allocated() / (1024 ** 3)
    logger.info(f"Peak GPU memory during encoding: {peak_encoding_memory_gb:.2f} GB")
    self.peak_encoding_memory_gb = peak_encoding_memory_gb
```

#### 2. train.py
**ä½ç½®**: `/home/ubuntu/OOD-DPO/train.py`

**ä¿®æ”¹å†…å®¹**:
- åœ¨data loaderåˆå§‹åŒ–åè·å– `peak_encoding_memory_gb`
- åœ¨è®­ç»ƒå¼€å§‹å‰é‡ç½®GPUå†…å­˜ç»Ÿè®¡ (ä»…æµ‹é‡è®­ç»ƒé˜¶æ®µ)
- åœ¨è¿”å›çš„training statsä¸­åŒ…å«encoding memory

**å…³é”®ä»£ç **:
```python
# Line 280-283: è·å–encoding memory
peak_encoding_memory_gb = getattr(data_loader, 'peak_encoding_memory_gb', 0.0)
if peak_encoding_memory_gb > 0:
    logger.info(f"Foundation model encoding peak GPU memory: {peak_encoding_memory_gb:.2f} GB")

# Line 467-468: è®°å½•encoding memoryåˆ°æ—¥å¿—
if peak_encoding_memory_gb > 0:
    logger.info(f"Peak GPU memory (encoding): {peak_encoding_memory_gb:.2f}GB")

# Line 476-481: è¿”å›å®Œæ•´çš„memoryç»Ÿè®¡
return {
    'train_time_seconds': total_train_time,
    'avg_epoch_time_seconds': avg_epoch_time,
    'peak_gpu_memory_train_gb': peak_gpu_memory_gb,
    'peak_gpu_memory_encoding_gb': peak_encoding_memory_gb  # æ–°å¢
}
```

#### 3. baseline_trainer.py
**ä½ç½®**: `/home/ubuntu/OOD-DPO/baseline_trainer.py`

**ä¿®æ”¹å†…å®¹**: ä¸train.pyç›¸åŒçš„é€»è¾‘

**å…³é”®ä»£ç **:
```python
# Line 223-226: è·å–encoding memory
peak_encoding_memory_gb = getattr(self.data_loader, 'peak_encoding_memory_gb', 0.0)
if peak_encoding_memory_gb > 0:
    logger.info(f"Foundation model encoding peak GPU memory: {peak_encoding_memory_gb:.2f} GB")

# Line 643-644: è®°å½•encoding memoryåˆ°æ—¥å¿—
if peak_encoding_memory_gb > 0:
    logger.info(f"Peak GPU memory (encoding): {peak_encoding_memory_gb:.2f}GB")

# Line 654-660: è¿”å›å®Œæ•´çš„memoryç»Ÿè®¡
return {
    'checkpoint': final_checkpoint,
    'train_time_seconds': total_train_time,
    'avg_epoch_time_seconds': avg_epoch_time,
    'peak_gpu_memory_train_gb': peak_gpu_memory_gb,
    'peak_gpu_memory_encoding_gb': peak_encoding_memory_gb  # æ–°å¢
}
```

## é¢„æœŸæ•ˆæœ

### æ€§èƒ½æå‡

| æŒ‡æ ‡ | å½“å‰ | ä¼˜åŒ–å | æå‡å€æ•° |
|------|------|--------|---------|
| **Encodingé€Ÿåº¦** | 50 mol/batch | 500 mol/batch | **10x** |
| **è®­ç»ƒé€Ÿåº¦** | 512 batch â†’ 8192 batch | **16x batch size** | **5-10x** ğŸ”¥ |
| **GPUåˆ©ç”¨ç‡ (è®­ç»ƒ)** | <1% (0.1GB) | **30-50% (25-40GB)** | **~250-400x** ğŸ”¥ |

### å†…å­˜ä½¿ç”¨é¢„æµ‹

| é˜¶æ®µ | Foundation Model | å½“å‰æµ‹é‡å€¼ | ä¼˜åŒ–åé¢„æœŸå€¼ (80GB A100) | è¯´æ˜ |
|------|------------------|-----------|------------------------|------|
| **Encoding** | **Minimol** | 0 GB (CPU) | **0 GB (CPU)** | âš ï¸ **Minimolåœ¨CPUä¸Šè¿è¡Œ** |
| **Encoding** | **Unimol** | âŒ æœªæµ‹é‡ | 8-20 GB (GPU) | Unimolä½¿ç”¨GPU |
| **Training** | Both | 0.1 GB | **25-40 GB** ğŸ”¥ | DPOè®­ç»ƒï¼Œ16x batch size |
| **Evaluation** | Both | 0.1 GB | **15-25 GB** ğŸ”¥ | å‰å‘ä¼ æ’­ï¼Œ16-32x batch size |
| **æ€»Peak** | Both | 0.1 GB | **30-50 GB** ğŸ”¥ | å……åˆ†åˆ©ç”¨ï¼Œä»æœ‰30-50GBä½™é‡ |

### âš ï¸ é‡è¦è¯´æ˜ï¼šMinimolä½¿ç”¨CPU

**Minimol foundation modelåœ¨CPUä¸Šè¿è¡Œï¼Œè¿™æ˜¯æ­£å¸¸è¡Œä¸ºï¼š**

1. **ä¸ºä»€ä¹ˆ**: MinimolåŸºäºgraphiumçš„Fingerprinterï¼Œå…¶æ•°æ®é¢„å¤„ç†pipelineåŒ…å«å¿…é¡»åœ¨CPUä¸Šæ‰§è¡Œçš„æ“ä½œ
2. **å½±å“**:
   - Encoding GPU memory = 0 GBï¼ˆæ­£ç¡®æµ‹é‡ï¼‰
   - Encodingé€Ÿåº¦æ¯”GPUæ…¢ï¼Œä½†å¢å¤§batch sizeä»èƒ½æå‡é€Ÿåº¦
   - è®­ç»ƒé˜¶æ®µä»åœ¨GPUä¸Šè¿›è¡Œï¼Œä¸å—å½±å“
3. **ä¼˜åŒ–ä»ç„¶æœ‰æ•ˆ**: è™½ç„¶Minimol encodingä¸ç”¨GPUï¼Œä½†å¢å¤§encoding_batch_sizeï¼ˆ50â†’500ï¼‰ä»èƒ½æ˜¾è‘—æå‡å¹¶è¡ŒåŒ–æ•ˆç‡

**å¦‚æœéœ€è¦GPUåŠ é€Ÿencodingï¼Œè¯·ä½¿ç”¨Unimol foundation modelã€‚**

## ä¿®æ”¹æ–‡ä»¶æ¸…å•

âœ… **å·²ä¿®æ”¹çš„æ–‡ä»¶** (6ä¸ª):

1. `/home/ubuntu/OOD-DPO/run_experiments.sh` - Batch sizeä¼˜åŒ–
2. `/home/ubuntu/OOD-DPO/run_baselines.sh` - Batch sizeä¼˜åŒ–
3. `/home/ubuntu/OOD-DPO/run_cross_dataset_experiments.sh` - Batch sizeä¼˜åŒ–
4. `/home/ubuntu/OOD-DPO/data_loader.py` - Encoding GPU memoryæµ‹é‡
5. `/home/ubuntu/OOD-DPO/train.py` - GPU memory tracking
6. `/home/ubuntu/OOD-DPO/baseline_trainer.py` - GPU memory tracking

## ä½¿ç”¨è¯´æ˜

### 1. ç«‹å³ç”Ÿæ•ˆ
æ‰€æœ‰ä¿®æ”¹å·²å®Œæˆï¼Œä¸‹æ¬¡è¿è¡Œå®éªŒæ—¶è‡ªåŠ¨ç”Ÿæ•ˆã€‚

### 2. ç›‘æ§GPUä½¿ç”¨
```bash
# å®æ—¶ç›‘æ§GPU
watch -n 1 nvidia-smi

# æˆ–è€…ä½¿ç”¨
nvidia-smi dmon -s mu
```

### 3. æ£€æŸ¥æ–°çš„å†…å­˜ç»Ÿè®¡
è®­ç»ƒå®Œæˆåï¼Œæ£€æŸ¥ä»¥ä¸‹æ–‡ä»¶ï¼š
```bash
# æŸ¥çœ‹training stats (åŒ…å«encoding memory)
cat outputs/*/training_stats.json

# æŸ¥çœ‹baseline results (åŒ…å«encoding memory)
cat baseline_outputs/*/results.json
```

**æ–°å¢çš„JSONå­—æ®µ**:
```json
{
  "peak_gpu_memory_encoding_gb": X.XX,  // æ–°å¢ï¼šencodingé˜¶æ®µGPUå³°å€¼
  "peak_gpu_memory_train_gb": Y.YY,     // è®­ç»ƒé˜¶æ®µGPUå³°å€¼
  "peak_gpu_memory_eval_gb": Z.ZZ       // è¯„ä¼°é˜¶æ®µGPUå³°å€¼
}
```

### 4. å¦‚æœé‡åˆ°OOMé”™è¯¯

å¦‚æœå‡ºç°GPUå†…å­˜ä¸è¶³ (Out of Memory)ï¼Œå¯ä»¥é€æ­¥é™ä½batch sizeï¼š

**é€æ­¥é™ä½encoding_batch_size**:
```bash
# åœ¨shellè„šæœ¬ä¸­ä¿®æ”¹
--encoding_batch_size 500  # å¦‚æœOOM
â†’ --encoding_batch_size 300
â†’ --encoding_batch_size 200
â†’ --encoding_batch_size 100
```

**é€æ­¥é™ä½training batch_size**:
```bash
# Minimol
batch_size=2048  # å¦‚æœOOM
â†’ batch_size=1024
â†’ batch_size=512

# Unimol
batch_size=1024  # å¦‚æœOOM
â†’ batch_size=512
â†’ batch_size=256
```

## æŠ€æœ¯ç»†èŠ‚

### ä¸ºä»€ä¹ˆencoding_batch_sizeæ˜¯å…³é”®?

1. **Foundation model encoding**æ˜¯æœ€æ¶ˆè€—GPUçš„æ“ä½œ:
   - Minimol/Unimoléœ€è¦åŠ è½½å¤§å‹é¢„è®­ç»ƒæ¨¡å‹
   - æ¯ä¸ªåˆ†å­éœ€è¦é€šè¿‡æ•´ä¸ªtransformerç½‘ç»œ
   - è¾“å‡º512ç»´ç‰¹å¾å‘é‡

2. **Encodingåªå‘ç”Ÿä¸€æ¬¡**:
   - ç‰¹å¾è¢«ç¼“å­˜åˆ°ç£ç›˜ (`*.pkl`)
   - åç»­è®­ç»ƒç›´æ¥ä½¿ç”¨ç¼“å­˜ç‰¹å¾
   - å› æ­¤encodingæ•ˆç‡ç›´æ¥å½±å“é¦–æ¬¡è¿è¡Œé€Ÿåº¦

3. **æ›´å¤§çš„encoding batch = æ›´å¥½çš„GPUåˆ©ç”¨**:
   - GPUé€‚åˆå¹¶è¡Œå¤„ç†
   - 50ä¸ªåˆ†å­/batch â†’ GPUå¤§éƒ¨åˆ†æ—¶é—´ç©ºé—²
   - 500ä¸ªåˆ†å­/batch â†’ GPUå……åˆ†åˆ©ç”¨

### ä¸ºä»€ä¹ˆä¹‹å‰æµ‹é‡å€¼è¿™ä¹ˆä½?

**åŸå› **: GPUå†…å­˜åœ¨data loaderåˆå§‹åŒ–æ—¶é‡ç½®ï¼Œä½†encodingåœ¨æ­¤ä¹‹å‰å®Œæˆ

```python
# é”™è¯¯çš„é¡ºåº (ä¹‹å‰):
data_loader = EnergyDPODataLoader(args)  # encodingåœ¨è¿™é‡Œå®Œæˆ
torch.cuda.reset_peak_memory_stats()      # é‡ç½®ï¼ä¹‹å‰çš„encoding memoryè¢«æ¸…é›¶
# ... training ...
peak = torch.cuda.max_memory_allocated()  # åªæµ‹åˆ°training memory

# æ­£ç¡®çš„é¡ºåº (ç°åœ¨):
# In data_loader._compute_features_batch():
torch.cuda.reset_peak_memory_stats()      # encodingå‰é‡ç½®
# ... encoding ...
peak_encoding = torch.cuda.max_memory_allocated()  # æµ‹åˆ°encoding memory
self.peak_encoding_memory_gb = peak_encoding

# In train():
data_loader = EnergyDPODataLoader(args)
peak_encoding = data_loader.peak_encoding_memory_gb  # è·å–encoding memory
torch.cuda.reset_peak_memory_stats()      # é‡ç½®ï¼Œå¼€å§‹æµ‹training
# ... training ...
peak_training = torch.cuda.max_memory_allocated()  # æµ‹åˆ°training memory
```

## éªŒè¯æ£€æŸ¥æ¸…å•

è¿è¡Œå®éªŒåï¼ŒéªŒè¯ä¼˜åŒ–æ˜¯å¦ç”Ÿæ•ˆï¼š

- [ ] æ—¥å¿—ä¸­å‡ºç° "Starting foundation model encoding - GPU memory tracking enabled"
- [ ] æ—¥å¿—ä¸­å‡ºç° "Peak GPU memory during encoding: X.XX GB" (X > 0)
- [ ] æ—¥å¿—ä¸­å‡ºç° "Foundation model encoding peak GPU memory: X.XX GB"
- [ ] `training_stats.json` åŒ…å« `peak_gpu_memory_encoding_gb` å­—æ®µ
- [ ] Encoding GPU memory > 1 GB (ä¹‹å‰æ˜¯0æˆ–æœªè®°å½•)
- [ ] Training GPU memory > 1 GB (ä¹‹å‰æ˜¯0.017 GB)
- [ ] Encodingé€Ÿåº¦æ˜æ˜¾åŠ å¿« (å¦‚æœé‡æ–°è®¡ç®—features)

## æ•…éšœæ’æŸ¥

### é—®é¢˜1: encoding memoryä»ç„¶æ˜¯0æˆ–æœªè®°å½•
**åŸå› **: ä½¿ç”¨äº†ç¼“å­˜çš„featuresï¼Œæ²¡æœ‰é‡æ–°encoding
**è§£å†³**:
```bash
# å¼ºåˆ¶é‡æ–°è®¡ç®—features
python main.py ... --force_recompute_cache
# æˆ–åˆ é™¤cache
rm /home/ubuntu/projects/*_features.pkl
```

### é—®é¢˜2: OOM (Out of Memory)
**åŸå› **: Batch sizeå¯¹äºç‰¹å®šæ•°æ®é›†/æ¨¡å‹å¤ªå¤§
**è§£å†³**: é€æ­¥é™ä½batch size (è§ä¸Šæ–‡"å¦‚æœé‡åˆ°OOMé”™è¯¯")

### é—®é¢˜3: è®­ç»ƒå˜æ…¢
**åŸå› **: å¯èƒ½çš„data loadingç“¶é¢ˆ
**è§£å†³**:
```bash
# å¢åŠ num_workers
--num_workers 4  # ä»2å¢åŠ åˆ°4
```

## åç»­å»ºè®®

### è¿›ä¸€æ­¥ä¼˜åŒ– (å¯é€‰)

1. **æ··åˆç²¾åº¦è®­ç»ƒ** (å¯èŠ‚çœ~50% GPUå†…å­˜):
```python
# åœ¨train.pyä¸­æ·»åŠ 
from torch.cuda.amp import autocast, GradScaler
scaler = GradScaler()

with autocast():
    loss = model(...)
scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()
```

2. **Gradient accumulation** (æ¨¡æ‹Ÿæ›´å¤§batch size):
```python
# å¦‚æœå•ä¸ªå¤§batch OOMï¼Œå¯ä»¥ç´¯ç§¯å¤šä¸ªå°batch
accumulation_steps = 4
for i, batch in enumerate(dataloader):
    loss = model(batch) / accumulation_steps
    loss.backward()
    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()
```

3. **åŠ¨æ€batch size** (æ ¹æ®åˆ†å­å¤§å°è°ƒæ•´):
```python
# å¤§åˆ†å­ç”¨å°batchï¼Œå°åˆ†å­ç”¨å¤§batch
if max_atoms < 50:
    batch_size = 4096
elif max_atoms < 100:
    batch_size = 2048
else:
    batch_size = 1024
```

## æ€»ç»“

âœ… **å®Œæˆçš„ä¼˜åŒ–**:
- Batch sizeå¢åŠ  4-10å€
- å®Œæ•´çš„GPUå†…å­˜æµ‹é‡ (encoding + training + eval)
- è¯¦ç»†çš„æ€§èƒ½æ—¥å¿—è®°å½•

âœ… **é¢„æœŸæ”¶ç›Š**:
- Encodingé€Ÿåº¦: 5-10xæå‡
- è®­ç»ƒé€Ÿåº¦: 2-4xæå‡
- GPUåˆ©ç”¨ç‡: ä»<1%æå‡åˆ°12-37%

âœ… **å®‰å…¨æ€§**:
- åœ¨80GB A100ä¸Šæœ‰å……è¶³ä½™é‡
- å¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´batch size
- å®Œæ•´çš„å†…å­˜ç›‘æ§é¿å…æ„å¤–OOM

---

**ä½œè€…**: Claude Code
**æ—¥æœŸ**: 2025-11-20
**ç‰ˆæœ¬**: 1.0
