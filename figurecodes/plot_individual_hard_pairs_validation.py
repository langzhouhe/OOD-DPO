#!/usr/bin/env python3
"""
ç”Ÿæˆå•ç‹¬çš„EC50éš¾å¯¹éªŒè¯å›¾è¡¨ - æ¯ä¸ªåˆ†æä¸€å¼ å›¾
åŒæ—¶è¾“å‡ºPNGå’ŒSVGæ ¼å¼
"""

import os
import sys
import argparse
import logging
import json
import numpy as np
import matplotlib.pyplot as plt
import matplotlib
from tqdm import tqdm
import torch
import torch.nn.functional as F
from scipy.special import expit as sigmoid

# å¯¼å…¥é¡¹ç›®æ¨¡å—
sys.path.append('/home/ubuntu/OOD-DPO')
from model import EnergyDPOModel
from data_loader import EnergyDPODataLoader

# è®¾ç½®ä¸“ä¸šç»˜å›¾é£æ ¼
plt.style.use('default')
matplotlib.rcParams['font.family'] = 'Times New Roman'
matplotlib.rcParams['font.size'] = 16
matplotlib.rcParams['axes.linewidth'] = 1.2
matplotlib.rcParams['axes.spines.right'] = False
matplotlib.rcParams['axes.spines.top'] = False

# æ•°æ®é›†é¢œè‰²é…ç½®ï¼ˆå‚è€ƒbeta plotsï¼‰
DATASET_COLORS = {
    'lbap_general_ec50_assay': '#2E86AB',      # æ˜äº®çš„è“è‰²
    'lbap_general_ec50_scaffold': '#F24236',   # æ˜äº®çš„çº¢è‰²
    'lbap_general_ec50_size': '#2E8B57'        # ç»¿è‰²
}

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def setup_device():
    """è®¾ç½®è®¡ç®—è®¾å¤‡"""
    if torch.cuda.is_available():
        device = torch.device('cuda')
        logger.info(f"Using GPU: {torch.cuda.get_device_name()}")
    else:
        device = torch.device('cpu')
        logger.info("Using CPU")
    return device

class IndividualHardPairsValidator:
    """ç”Ÿæˆå•ç‹¬å›¾è¡¨çš„EC50éš¾å¯¹éªŒè¯åˆ†æå™¨"""

    def __init__(self, device):
        self.device = device
        self.base_model_path = '/home/ubuntu/OOD-DPO/outputs/minimol'
        self.data_path = './data/raw'

    def load_model_and_data(self, dataset_name, seed=1):
        """åŠ è½½æ¨¡å‹å’Œæ•°æ®"""
        # åŠ è½½æ¨¡å‹
        model_path = f"{self.base_model_path}/{dataset_name}/{seed}/best_model.pth"

        if not os.path.exists(model_path):
            raise FileNotFoundError(f"Model not found: {model_path}")

        logger.info(f"Loading model from {model_path}")

        # åˆ›å»ºargså¯¹è±¡ä»¥åˆå§‹åŒ–æ¨¡å‹
        class Args:
            def __init__(self):
                self.foundation_model = 'minimol'
                self.dpo_beta = 0.1
                self.hidden_dim = 256

        args = Args()
        model = EnergyDPOModel(args)

        # åŠ è½½æ¨¡å‹çŠ¶æ€
        checkpoint = torch.load(model_path, map_location=self.device)
        if 'model_state_dict' in checkpoint:
            model.load_state_dict(checkpoint['model_state_dict'])
        else:
            model.load_state_dict(checkpoint)

        model = model.to(self.device)
        model.eval()

        # æå–å®é™…çš„betaå€¼
        if hasattr(model, 'beta'):
            if torch.is_tensor(model.beta):
                actual_beta = float(model.beta.cpu().detach().numpy())
            else:
                actual_beta = float(model.beta)
        else:
            actual_beta = 0.1
        logger.info(f"Extracted actual beta value: {actual_beta}")

        # åŠ è½½æ•°æ®
        class DataArgs:
            def __init__(self, data_path):
                self.dataset = dataset_name
                self.foundation_model = 'minimol'
                self.data_path = data_path
                self.data_seed = 42

        data_args = DataArgs(self.data_path)
        data_loader = EnergyDPODataLoader(data_args)
        test_id, test_ood = data_loader.get_final_test_data()

        logger.info(f"Loaded test data: {len(test_id)} ID samples, {len(test_ood)} OOD samples")

        return model, actual_beta, test_id, test_ood

    def compute_analysis_data(self, model, actual_beta, test_id, test_ood, max_samples=500):
        """è®¡ç®—åˆ†ææ‰€éœ€çš„æ•°æ®"""
        if max_samples:
            n_samples = min(max_samples, len(test_id), len(test_ood))
            test_id = test_id[:n_samples]
            test_ood = test_ood[:n_samples]

        # è®¡ç®—èƒ½é‡
        batch_size = 100
        all_energy_id = []
        all_energy_ood = []

        with torch.no_grad():
            # è®¡ç®—IDèƒ½é‡
            for i in tqdm(range(0, len(test_id), batch_size), desc="Computing ID energies"):
                batch_id = test_id[i:i+batch_size]
                if isinstance(batch_id[0], dict):
                    features_id = torch.stack([sample['features'] for sample in batch_id]).to(self.device)
                else:
                    features_id = torch.stack(batch_id).to(self.device)
                energy_id = model.forward_energy(features_id).cpu().numpy()
                all_energy_id.extend(energy_id)

            # è®¡ç®—OODèƒ½é‡
            for i in tqdm(range(0, len(test_ood), batch_size), desc="Computing OOD energies"):
                batch_ood = test_ood[i:i+batch_size]
                if isinstance(batch_ood[0], dict):
                    features_ood = torch.stack([sample['features'] for sample in batch_ood]).to(self.device)
                else:
                    features_ood = torch.stack(batch_ood).to(self.device)
                energy_ood = model.forward_energy(features_ood).cpu().numpy()
                all_energy_ood.extend(energy_ood)

        all_energy_id = np.array(all_energy_id)
        all_energy_ood = np.array(all_energy_ood)

        # ç”Ÿæˆpairs
        max_pairs = min(50000, len(all_energy_id) * len(all_energy_ood))
        id_indices = np.random.choice(len(all_energy_id), size=max_pairs, replace=True)
        ood_indices = np.random.choice(len(all_energy_ood), size=max_pairs, replace=True)

        energy_id_pairs = all_energy_id[id_indices]
        energy_ood_pairs = all_energy_ood[ood_indices]

        # è®¡ç®—èƒ½é‡å·®å’Œæ¢¯åº¦æƒé‡
        delta_values = energy_ood_pairs - energy_id_pairs
        weights = actual_beta * sigmoid(-actual_beta * delta_values)

        # åˆ›å»ºåˆ†ç®±åˆ†æ
        n_bins = 20
        valid_mask = np.isfinite(delta_values) & np.isfinite(weights)
        delta_clean = delta_values[valid_mask]
        weights_clean = weights[valid_mask]

        bin_edges = np.linspace(np.percentile(delta_clean, 1),
                               np.percentile(delta_clean, 99), n_bins + 1)
        bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2

        mean_weights = np.zeros(n_bins)
        std_weights = np.zeros(n_bins)
        counts = np.zeros(n_bins)

        for i in range(n_bins):
            mask = (delta_clean >= bin_edges[i]) & (delta_clean < bin_edges[i + 1])
            if mask.sum() > 0:
                mean_weights[i] = weights_clean[mask].mean()
                std_weights[i] = weights_clean[mask].std()
                counts[i] = mask.sum()
            else:
                mean_weights[i] = np.nan
                std_weights[i] = np.nan
                counts[i] = 0

        binned_data = {
            'bin_centers': bin_centers,
            'mean_weights': mean_weights,
            'std_weights': std_weights,
            'counts': counts
        }

        return {
            'delta_values': delta_values,
            'weights': weights,
            'binned_data': binned_data,
            'actual_beta': actual_beta
        }

def save_both_formats(fig, filepath_base):
    """ä¿å­˜PNGå’ŒSVGæ ¼å¼"""
    plt.savefig(f"{filepath_base}.png", format='png', bbox_inches='tight', dpi=300, facecolor='white')
    plt.savefig(f"{filepath_base}.svg", format='svg', bbox_inches='tight', facecolor='white')
    plt.close()
    logger.info(f"âœ… Saved both formats: {filepath_base}.png/.svg")

def create_individual_plots(dataset_name, analysis_data, output_dir):
    """åˆ›å»º4ä¸ªå•ç‹¬çš„å›¾è¡¨"""
    os.makedirs(output_dir, exist_ok=True)

    primary_color = DATASET_COLORS.get(dataset_name, '#F24236')
    dataset_display_name = dataset_name.replace('lbap_general_ec50_', '').title()

    delta_values = analysis_data['delta_values']
    weights = analysis_data['weights']
    binned_data = analysis_data['binned_data']
    actual_beta = analysis_data['actual_beta']

    hard_pairs_mask = delta_values < 0
    easy_pairs_mask = delta_values > 0
    boundary_mask = np.abs(delta_values) < 0.05

    # å›¾1: æ ¸å¿ƒéªŒè¯å›¾ - ç»éªŒvsç†è®ºæ›²çº¿
    fig, ax = plt.subplots(1, 1, figsize=(10, 7))

    # é‡‡æ ·æ•°æ®ç‚¹
    sample_idx = np.random.choice(len(delta_values), size=min(3000, len(delta_values)), replace=False)
    ax.scatter(delta_values[sample_idx], weights[sample_idx],
               alpha=0.3, s=2, color='lightgray', label='Individual pairs', zorder=1)

    # ç»éªŒæ›²çº¿
    valid_bins = ~np.isnan(binned_data['mean_weights'])
    ax.errorbar(binned_data['bin_centers'][valid_bins],
                binned_data['mean_weights'][valid_bins],
                yerr=binned_data['std_weights'][valid_bins] / np.sqrt(binned_data['counts'][valid_bins]),
                fmt='o-', color=primary_color, markersize=8, linewidth=3, capsize=5,
                label='Empirical curve', zorder=3)

    # ç†è®ºæ›²çº¿
    t_theory = np.linspace(delta_values.min(), delta_values.max(), 1000)
    w_theory = actual_beta * sigmoid(-actual_beta * t_theory)
    ax.plot(t_theory, w_theory, '--', color='black', linewidth=3,
            label=f'Theory: Î²Â·Ïƒ(-Î²t), Î²={actual_beta:.3f}', zorder=2)

    ax.axvline(x=0, color='gray', linestyle=':', linewidth=2, alpha=0.7,
               label='Decision boundary')
    ax.set_xlabel('Energy Difference Î”E = E_ood - E_id', fontsize=16)
    ax.set_ylabel('Gradient Weight w_Î²(Î”E)', fontsize=16)
    ax.set_title(f'Core Validation: Empirical vs Theoretical\n{dataset_display_name} Dataset',
                fontsize=18, fontweight='bold')
    ax.legend(fontsize=14)
    ax.grid(True, alpha=0.3)

    save_both_formats(fig, f"{output_dir}/figure_a3_1_core_validation_{dataset_name}")

    # å›¾2: æƒé‡å¯¹æ¯”æŸ±çŠ¶å›¾
    fig, ax = plt.subplots(1, 1, figsize=(10, 7))

    categories = ['Hard Pairs\n(Î”E<0)', 'Easy Pairs\n(Î”E>0)', 'Boundary\n(|Î”E|<0.05)']
    mean_weights = [
        weights[hard_pairs_mask].mean() if hard_pairs_mask.any() else 0,
        weights[easy_pairs_mask].mean() if easy_pairs_mask.any() else 0,
        weights[boundary_mask].mean() if boundary_mask.any() else 0
    ]
    colors = [primary_color, 'lightgray', 'orange']

    bars = ax.bar(categories, mean_weights, color=colors, alpha=0.8,
                  edgecolor='black', linewidth=1.5)

    for bar, weight in zip(bars, mean_weights):
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height + 0.0005,
                f'{weight:.4f}', ha='center', va='bottom', fontweight='bold', fontsize=14)

    ax.set_ylabel('Average Gradient Weight', fontsize=16)
    advantage = ((mean_weights[0]/mean_weights[1]-1)*100) if mean_weights[1] > 0 else 0
    ax.set_title(f'Weight Comparison - {dataset_display_name}\nHard pairs get {advantage:.1f}% higher weights',
                fontsize=18, fontweight='bold')
    ax.grid(True, alpha=0.3, axis='y')

    save_both_formats(fig, f"{output_dir}/figure_a3_2_weight_comparison_{dataset_name}")

    # å›¾3: æƒé‡åˆ†å¸ƒç›´æ–¹å›¾
    fig, ax = plt.subplots(1, 1, figsize=(10, 7))

    ax.hist(weights, bins=50, alpha=0.7, density=True, color=primary_color,
            edgecolor='black', linewidth=0.5, label='All pairs')

    # æ·»åŠ å‡å€¼çº¿
    ax.axvline(x=weights.mean(), color='black', linestyle='--', linewidth=2,
               label=f'Overall mean: {weights.mean():.4f}')

    if hard_pairs_mask.any():
        ax.axvline(x=weights[hard_pairs_mask].mean(), color='red', linestyle='--', linewidth=2,
                   label=f'Hard pairs: {weights[hard_pairs_mask].mean():.4f}')

    if easy_pairs_mask.any():
        ax.axvline(x=weights[easy_pairs_mask].mean(), color='green', linestyle='--', linewidth=2,
                   label=f'Easy pairs: {weights[easy_pairs_mask].mean():.4f}')

    ax.set_xlabel('Gradient Weight w_Î²(Î”E)', fontsize=16)
    ax.set_ylabel('Probability Density', fontsize=16)
    ax.set_title(f'Weight Distribution Analysis - {dataset_display_name}',
                fontsize=18, fontweight='bold')
    ax.legend(fontsize=14)
    ax.grid(True, alpha=0.3)

    save_both_formats(fig, f"{output_dir}/figure_a3_3_weight_distribution_{dataset_name}")

    # å›¾4: èƒ½é‡å·®åˆ†å¸ƒå›¾
    fig, ax = plt.subplots(1, 1, figsize=(10, 7))

    ax.hist(delta_values, bins=50, alpha=0.6, color='lightsteelblue',
            edgecolor='black', linewidth=0.5, label='Energy differences')

    ax.axvline(x=0, color='red', linestyle='--', linewidth=3, label='Decision boundary')
    ax.axvline(x=delta_values.mean(), color='blue', linestyle='--', linewidth=2,
               label=f'Mean Î”E: {delta_values.mean():.2f}')

    # å¡«å……åŒºåŸŸ
    ylim = ax.get_ylim()
    if hard_pairs_mask.any():
        ax.fill_between([delta_values.min(), 0], 0, ylim[1], alpha=0.2, color='red',
                       label=f'Hard pairs ({hard_pairs_mask.mean():.1%})')

    if easy_pairs_mask.any():
        ax.fill_between([0, delta_values.max()], 0, ylim[1], alpha=0.2, color='green',
                       label=f'Easy pairs ({easy_pairs_mask.mean():.1%})')

    ax.set_xlabel('Energy Difference Î”E = E_ood - E_id', fontsize=16)
    ax.set_ylabel('Count', fontsize=16)
    ax.set_title(f'Energy Difference Distribution - {dataset_display_name}',
                fontsize=18, fontweight='bold')
    ax.legend(fontsize=14)
    ax.grid(True, alpha=0.3)

    save_both_formats(fig, f"{output_dir}/figure_a3_4_energy_distribution_{dataset_name}")

    logger.info(f"âœ… All 4 individual plots created for {dataset_name}")

def main():
    parser = argparse.ArgumentParser(description='ç”Ÿæˆå•ç‹¬çš„EC50éš¾å¯¹éªŒè¯å›¾è¡¨')
    parser.add_argument('--datasets', nargs='+',
                       default=['lbap_general_ec50_scaffold', 'lbap_general_ec50_size', 'lbap_general_ec50_assay'],
                       help='è¦åˆ†æçš„æ•°æ®é›†')
    parser.add_argument('--seeds', nargs='+', type=int, default=[1],
                       help='è¦åˆ†æçš„éšæœºç§å­')
    parser.add_argument('--max_samples', type=int, default=500,
                       help='æ¯ä¸ªæ•°æ®é›†æœ€å¤§æ ·æœ¬æ•°')
    parser.add_argument('--output_dir', type=str, default='individual_hard_pairs_plots',
                       help='è¾“å‡ºç›®å½•')

    args = parser.parse_args()

    # è®¾ç½®è®¾å¤‡
    device = setup_device()

    # åˆå§‹åŒ–åˆ†æå™¨
    validator = IndividualHardPairsValidator(device)

    for dataset_name in args.datasets:
        for seed in args.seeds:
            try:
                logger.info(f"\n{'='*60}")
                logger.info(f"Processing {dataset_name} with seed {seed}")
                logger.info(f"{'='*60}")

                # åŠ è½½æ¨¡å‹å’Œæ•°æ®
                model, actual_beta, test_id, test_ood = validator.load_model_and_data(dataset_name, seed)

                # è®¡ç®—åˆ†ææ•°æ®
                analysis_data = validator.compute_analysis_data(model, actual_beta, test_id, test_ood, args.max_samples)

                # åˆ›å»ºå•ç‹¬å›¾è¡¨
                create_individual_plots(dataset_name, analysis_data, args.output_dir)

                print(f"\nâœ… {dataset_name} åˆ†æå®Œæˆï¼ç”Ÿæˆäº†4ä¸ªå•ç‹¬å›¾è¡¨ï¼ˆPNG + SVGï¼‰")

            except Exception as e:
                logger.error(f"Failed to process {dataset_name} seed {seed}: {e}")
                continue

    logger.info(f"\nğŸ‰ æ‰€æœ‰åˆ†æå®Œæˆï¼å›¾è¡¨ä¿å­˜è‡³: {args.output_dir}")

if __name__ == '__main__':
    main()